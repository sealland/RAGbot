# src/app.py (Final Correct Version)

import os
import streamlit as st
from dotenv import load_dotenv
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings  # <--- à¹à¸à¹‰à¹„à¸‚ 1
from langchain_postgres.vectorstores import PGVector

# --- 1. à¹‚à¸«à¸¥à¸” Environment Variables à¹à¸¥à¸°à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¸žà¸·à¹‰à¸™à¸à¸²à¸™ ---
load_dotenv()

CONNECTION_STRING = PGVector.connection_string_from_db_params(
    driver="psycopg",
    host=os.environ.get("POSTGRES_HOST"),
    port=int(os.environ.get("POSTGRES_PORT")),
    database=os.environ.get("POSTGRES_DB"),
    user=os.environ.get("POSTGRES_USER"),
    password=os.environ.get("POSTGRES_PASSWORD"),
)
COLLECTION_NAME = "machine_repair_docs"

# --- 2. à¸ªà¸£à¹‰à¸²à¸‡ Prompt Template ---
template = """
à¸„à¸¸à¸“à¸„à¸·à¸­à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢ AI à¹€à¸Šà¸µà¹ˆà¸¢à¸§à¸Šà¸²à¸à¸”à¹‰à¸²à¸™à¸à¸²à¸£à¸‹à¹ˆà¸­à¸¡à¸šà¸³à¸£à¸¸à¸‡à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸ˆà¸±à¸à¸£à¸‚à¸­à¸‡à¸šà¸£à¸´à¸©à¸±à¸—
à¸ˆà¸‡à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰à¸šà¸£à¸´à¸šà¸— (Context) à¸—à¸µà¹ˆà¹ƒà¸«à¹‰à¸¡à¸²à¹€à¸—à¹ˆà¸²à¸™à¸±à¹‰à¸™ à¸«à¹‰à¸²à¸¡à¹€à¸ªà¸£à¸´à¸¡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸£à¸·à¸­à¸•à¸­à¸šà¸ˆà¸²à¸à¸„à¸§à¸²à¸¡à¸£à¸¹à¹‰à¸­à¸·à¹ˆà¸™
à¸«à¸²à¸à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸™à¸šà¸£à¸´à¸šà¸—à¹„à¸¡à¹ˆà¹€à¸žà¸µà¸¢à¸‡à¸žà¸­à¸—à¸µà¹ˆà¸ˆà¸°à¸•à¸­à¸šà¸„à¸³à¸–à¸²à¸¡ à¹ƒà¸«à¹‰à¸•à¸­à¸šà¸§à¹ˆà¸² "à¸‰à¸±à¸™à¹„à¸¡à¹ˆà¸žà¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¹€à¸£à¸·à¹ˆà¸­à¸‡à¸™à¸µà¹‰à¹ƒà¸™à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¸„à¸£à¸±à¸š/à¸„à¹ˆà¸°"
à¸ˆà¸‡à¸•à¸­à¸šà¹ƒà¸«à¹‰à¸à¸£à¸°à¸Šà¸±à¸šà¹à¸¥à¸°à¹€à¸›à¹‡à¸™à¸‚à¸±à¹‰à¸™à¸•à¸­à¸™à¸—à¸µà¹ˆà¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸‡à¹ˆà¸²à¸¢

Context:
{context}

Question:
{question}

Answer:
"""
prompt = ChatPromptTemplate.from_template(template)

# --- 3. à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™à¸ªà¸³à¸«à¸£à¸±à¸šà¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¹à¸¥à¸° Retriever (à¹ƒà¸Šà¹‰ Cache à¹€à¸žà¸·à¹ˆà¸­à¸›à¸£à¸°à¸ªà¸´à¸—à¸˜à¸´à¸ à¸²à¸ž) ---
@st.cache_resource
def get_llm():
    """à¹‚à¸«à¸¥à¸”à¹‚à¸¡à¹€à¸”à¸¥à¸ à¸²à¸©à¸²à¸ˆà¸²à¸ Google Gemini"""
    return ChatGoogleGenerativeAI(model="gemini-1.5-pro-latest", temperature=0.1 ,max_output_tokens=8192 )

@st.cache_resource
def get_retriever():
    """à¸ªà¸£à¹‰à¸²à¸‡ Retriever à¹€à¸žà¸·à¹ˆà¸­à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¹à¸¥à¸°à¸„à¹‰à¸™à¸«à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸ˆà¸²à¸ PGVector"""
    # --- à¹à¸à¹‰à¹„à¸‚ 2 ---
    embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")

    # à¸ªà¸£à¹‰à¸²à¸‡à¸à¸²à¸£à¹€à¸Šà¸·à¹ˆà¸­à¸¡à¸•à¹ˆà¸­à¹„à¸›à¸¢à¸±à¸‡ Vector Store à¸—à¸µà¹ˆà¸¡à¸µà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§
    # --- à¹à¸à¹‰à¹„à¸‚ 3 (à¹ƒà¸Šà¹‰ 'embedding' à¹à¸¥à¸° 'connection') ---
    db = PGVector(
        embeddings=embeddings,  # <--- **à¹à¸à¹‰à¹„à¸‚à¹€à¸›à¹‡à¸™ `embeddings` (à¸¡à¸µ s)**
        collection_name=COLLECTION_NAME,
        connection=CONNECTION_STRING,
    )
    # à¸ªà¸£à¹‰à¸²à¸‡ Retriever à¸žà¸·à¹‰à¸™à¸à¸²à¸™à¸à¹ˆà¸­à¸™
    base_retriever = db.as_retriever(search_kwargs={'k': 3})  # k à¸—à¸µà¹ˆà¸™à¸µà¹ˆà¸­à¸²à¸ˆà¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹€à¸¢à¸­à¸°à¸¡à¸²à¸

    # à¸«à¹ˆà¸­à¸”à¹‰à¸§à¸¢ MultiQueryRetriever
    # à¸¡à¸±à¸™à¸ˆà¸°à¹ƒà¸Šà¹‰ llm à¸—à¸µà¹ˆà¹€à¸£à¸²à¸¡à¸µ (Gemini) à¹€à¸žà¸·à¹ˆà¸­à¸Šà¹ˆà¸§à¸¢à¹à¸•à¸à¸„à¸³à¸–à¸²à¸¡
    llm = get_llm()  # à¸”à¸¶à¸‡ llm à¸¡à¸²à¹ƒà¸Šà¹‰
    multi_query_retriever = MultiQueryRetriever.from_llm(
        retriever=base_retriever,
        llm=llm
    )
    return multi_query_retriever

def format_docs(docs):
    """à¸ˆà¸±à¸”à¸£à¸¹à¸›à¹à¸šà¸šà¹€à¸­à¸à¸ªà¸²à¸£à¸—à¸µà¹ˆà¸„à¹‰à¸™à¹€à¸ˆà¸­à¹ƒà¸«à¹‰à¸­à¸¢à¸¹à¹ˆà¹ƒà¸™à¸£à¸¹à¸›à¹à¸šà¸šà¸ªà¸•à¸£à¸´à¸‡à¹€à¸”à¸µà¸¢à¸§"""
    return "\n\n".join(doc.page_content for doc in docs)

# --- 4. à¸ªà¸£à¹‰à¸²à¸‡à¸«à¸™à¹‰à¸²à¹€à¸§à¹‡à¸šà¸”à¹‰à¸§à¸¢ Streamlit ---
st.set_page_config(page_title="ðŸ¤– à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢à¸Šà¹ˆà¸²à¸‡à¸‹à¹ˆà¸­à¸¡", layout="wide", initial_sidebar_state="collapsed")
st.title("ðŸ¤– à¸œà¸¹à¹‰à¸Šà¹ˆà¸§à¸¢à¸Šà¹ˆà¸²à¸‡à¸‹à¹ˆà¸­à¸¡à¸­à¸±à¸ˆà¸‰à¸£à¸´à¸¢à¸°")
st.markdown("à¸žà¸´à¸¡à¸žà¹Œà¸„à¸³à¸–à¸²à¸¡à¹€à¸à¸µà¹ˆà¸¢à¸§à¸à¸±à¸šà¸›à¸±à¸à¸«à¸²à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸ˆà¸±à¸à¸£ à¹à¸¥à¹‰à¸§à¸œà¸¡à¸ˆà¸°à¸„à¹‰à¸™à¸«à¸²à¸„à¸³à¸•à¸­à¸šà¸ˆà¸²à¸à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¸‹à¹ˆà¸­à¸¡à¸šà¸³à¸£à¸¸à¸‡à¹ƒà¸«à¹‰à¸„à¸£à¸±à¸š")

try:
    llm = get_llm()
    retriever = get_retriever()
    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if user_question := st.chat_input("à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸ˆà¸±à¸à¸£ XYZ à¸¡à¸µà¸›à¸±à¸à¸«à¸²à¸­à¸°à¹„à¸£?"):
        st.session_state.messages.append({"role": "user", "content": user_question})
        with st.chat_message("user"):
            st.markdown(user_question)
        with st.chat_message("assistant"):
            with st.spinner("à¸à¸³à¸¥à¸±à¸‡à¸„à¹‰à¸™à¸«à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸™à¸„à¸¹à¹ˆà¸¡à¸·à¸­à¹à¸¥à¸°à¹€à¸£à¸µà¸¢à¸šà¹€à¸£à¸µà¸¢à¸‡à¸„à¸³à¸•à¸­à¸š..."):
                response = rag_chain.invoke(user_question)
                st.markdown(response)
        st.session_state.messages.append({"role": "assistant", "content": response})

except Exception as e:
    st.error(f"à¹€à¸à¸´à¸”à¸‚à¹‰à¸­à¸œà¸´à¸”à¸žà¸¥à¸²à¸”à¹ƒà¸™à¸à¸²à¸£à¹€à¸£à¸´à¹ˆà¸¡à¸•à¹‰à¸™à¸£à¸°à¸šà¸š: {e}")
    st.error("à¸à¸£à¸¸à¸“à¸²à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¹ƒà¸™à¹„à¸Ÿà¸¥à¹Œ .env à¹à¸¥à¸°à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¹„à¸”à¹‰à¸£à¸±à¸™ ingest.py à¸ªà¸³à¹€à¸£à¹‡à¸ˆà¹à¸¥à¹‰à¸§")